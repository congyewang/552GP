---
title: "MATH552 Group Project"
author: "c.wang35@lancaster.ac.uk & k.molloy@lancaster.ac.uk"
---


```{r}
neomod <- read.table("../data/neomod_isam12.dat", header = T)

#plot(neomod)

summary(neomod)


# Attach to allow for better addressing
attach(neomod)

```


# Factorisation
Modify variables that are int or str to factor.
```{r}



neomod$cns    = factor(cns)
neomod$size   = factor(size)
neomod$gest   = factor(gest)
neomod$emp.f  = factor(emp.f)
neomod$emp.m  = factor(emp.m)
neomod$edu    = factor(edu)
neomod$re.ad  = factor(re.ad)
neomod$sex    = factor(sex)
neomod$accom  = factor(accom)


summary(neomod)

```
```{r}
head(neomod)
```
```{r}
# Representative Model
mod1 = glm(re.ad ~ bwt,
          data=neomod,
          family=binomial)



summary.glm(mod1)
```


```{r}
# Representative plot
par(mfrow=c(1,1))
plot(neomod$bwt,
     neomod$re.ad,
     xlab="Baseline Count",
     ylab="Fortnightly Count")
abline(mod1, col=2)
```

```{r}
# Representative plot
plot(mod1)
```

```{r}
# Random model - probably wrong
mod2.re.ad = glm(re.ad ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=binomial)

summary(mod2.re.ad)

plot(mod2.re.ad)
```
```{r}
# Random model - probably wrong
mod2.los = glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=Gamma(link=inverse))

summary(mod2.los)

plot(mod2.los)
```

```{r}
mod3.los = glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=gaussian)

summary.glm(mod3.los)

plot(mod3.los)
```

```{r}
# I think it is good.
mod4.los = glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=inverse.gaussian)

summary.glm(mod4.los)

plot(mod4.los)
```

```{r}
mod5.los = glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=poisson)

summary.glm(mod5.los)

plot(mod5.los)
```

```{r}
mod6.los = glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=quasi)

summary.glm(mod6.los)

plot(mod6.los)
```

```{r}
library(car)
```

This is an option plot.
It can create a comprehensive diagnostic diagram. The horizontal axis represents the lever value, the vertical axis represents the student residual value, and the size of the symbol drawn is directly proportional to the Cook distance.
  
# Model Diagnostics
```{r}
influencePlot(mod5.los)
```

This can test the overdispersion. If $\phi$ is greater than 1, it needs to be considered using Quasi-likelihood. See Chapter 5 in notes.

# Check Overdispersion
$$\phi = \frac{Residual\ \ Deviation}{Residual\ Degrees\ of\ Freedom}$$
```{r}
deviance(mod4.los)/df.residual(mod4.los)
```

# Model selection

```{r}
# Forward
mod.los = glm(los ~ 1,
                data= neomod,
                family=inverse.gaussian)

mod.los.for<-step(mod4.los, scope = list(upper=~cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,lower=~1), direction = "forward")
summary(mod.los.for)
```

```{r}
# Backward
mod.los <- glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=inverse.gaussian)

mod.los.back <- step(mod.los, direction="backward")
summary(mod.los.back)
```
Both the forward method and the backward method have obvious shortcomings. The forward method may have such a problem: it can not reflect the changes after the introduction of new independent variable values. Because the AIC corresponding to the regression equation is the minimum when an independent variable is introduced, but when other variables are introduced, the AIC value may be smaller if it is proposed from the regression equation, but there is no chance to put forward it by using the forward method, that is, once introduced, it will be lifelong. It is obviously not comprehensive to consider only the introduction but not the elimination. Similarly, once an independent variable is eliminated in the backward method, it has no chance to re-enter the regression equation.
```{r}
mod.los <- glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=inverse.gaussian)
mod.los.step <- step(mod.los, direction="both")
summary(mod.los.step)
```

## Model Comparison
```{r}
mod.los <- glm(los ~ cns + size + gest + bwt + emp.f + emp.m + edu + sex + accom,
           data= neomod,
           family=inverse.gaussian)

mod.los.step <- glm(los ~ cns + gest + bwt + emp.m + edu + sex, 
           data= neomod,
           family=inverse.gaussian)

anova(mod.los.step, mod.los, test = "Chisq")
```
